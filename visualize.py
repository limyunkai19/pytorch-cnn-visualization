import json, copy

import numpy as np
from PIL import Image
import matplotlib.cm as Pltcolormap
from matplotlib import pyplot as PLT

import torch, torchvision
import torch.nn.functional as F
from torchvision import transforms
from torch.autograd import Variable

import utils

class GradCAM:
    """
    Gradient-weighted Class Activation Mapping (Grad-CAM)
    Get a coarse heatmap of activation highlighting important location in
    input image based on the gradient of (last) convolutional layer to
    achieve "visual explanation" for CNN prediction

    Reference: https://arxiv.org/abs/1610.02391
    """

    def __init__(self, model, transform, target_layer, cuda=False):
        self.model = model
        self.model.train(False)
        self.cuda = cuda
        if self.cuda:
            self.model.cuda()
        self.transform = transform
        self.target_layer = target_layer

        # define hook function
        def forward_hook(module, input, output):
            self.feature_maps = output.data

        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0].data

        # register hook function
        for name, module in self.model.named_modules():
            if name == self.target_layer:
                module.register_forward_hook(forward_hook)
                module.register_backward_hook(backward_hook)

    def forward(self, img):
        """ The forward pass

        Argument:
            img (PIL Image) - the (unprocessed) input image

        Return:
            Tensor/Dict - the output of the model
        """

        # preprocess the PIL image first
        img_tensor = self.transform(img)
        img_tensor.unsqueeze_(0) # this add a dimension as a dummy "batch"
        img_variable = Variable(img_tensor)
        self.output = self.model(img_variable)
        return self.output.data

    def backward(self, idx, sorted_idx=False):
        self.model.zero_grad()
        self.output.backward(gradient=utils.one_hot_tensor(idx, 1000), retain_graph=True)

    def get_gradcam_intensity(self, idx, sorted_idx=False, backward=True):
        """ The (partial) backward pass and generate GradCAM intensity value for each pixel

        Argument:
            idx (int) - the idx of the class to be localize by GradCAM
            sorted_idx (bool) - if sorted_idx==True, the idx[0] will be the class with highest score,
                idx[1] will be the class with second highest score and so on
            backward (bool) - perform backward pass or not,
                                under normal usecase, it should be true

        Return:
            Tensor (size == kernal size of target layer)
                - GradCAM intensity value for each pixel
        """

        # implement sorted_idx !!!
        if backward:
            self.backward(idx)
        # self.feature_maps # 1x2048x7x7
        # self.gradients # 1x2048x7x7

        # GAP = torch.nn.AvgPool2d(self.gradients.size()[2:])
        weights = F.avg_pool2d(Variable(self.gradients), kernel_size=self.gradients.size()[2:]).data

        gradCAM_intensity = torch.FloatTensor(self.feature_maps.size()[2:]).zero_()

        for feature_map, weight in zip(self.feature_maps[0], weights[0]):
            gradCAM_intensity += feature_map * weight

        #relu
        gradCAM_intensity.clamp_(min=0)

        return gradCAM_intensity

    @staticmethod
    def apply_color_map(intensity, img):
        """ Apply the color map on the original image with GradCAM intensity value generated
            by GradCAM.backward()

        Argument:
            intensity (Tensor) - GradCAM intensity value generated by GradCAM.backward()
            img (PIL image) - The image that GradCAM intensity were to be apply to,
                suppose to be the original image

        Return:
            PIL image - The img with GradCAM intensity applied to
            Numpy array - The intensity same size as img (range: [0-1])
        """

        # normalize
        intensity = utils.normalize(intensity)

        # use PIL bilinear resize interpolation
        # note: *255 -> resize -> /255.0 (divide for heat map input[0,1]) is === resize
        pil = Image.fromarray(intensity.cpu().numpy())
        pil = pil.resize(img.size, resample=Image.BILINEAR)
        intensity = np.asarray(pil)

        # get the color map from matplotlib
        color_map = Pltcolormap.get_cmap('jet')
        heat_map = color_map(intensity)
        heat_map[:,:,3] /= 2.0
        heat_map *= 255

        original_img = np.asarray(img)

        return Image.fromarray(np.uint8((heat_map[:,:,:3]+original_img)/2.0)), intensity

class Backpropagation:
    """
    Vanilla Backpropagation
    Backpropagate to input then get the gradient at input
    """

    def __init__(self, model, transform, cuda=False):
        self.model = model
        # self.model = model
        self.model.train(False)
        self.cuda = cuda
        if self.cuda:
            self.model.cuda()
        self.transform = transform

    def forward(self, img):
        """ The forward pass

        Argument:
            img (PIL Image) - the (unprocessed) input image

        Return:
            Tensor/Dict - the output of the model
        """

        img_tensor = self.transform(img)
        img_tensor.unsqueeze_(0) # this add a dimension as a dummy "batch"
        img_variable = Variable(img_tensor, requires_grad=True)

        self.input = img_variable
        self.output = self.model(img_variable)
        return self.output.data

    def backward(self, idx):
        self.model.zero_grad()
        self.output.backward(gradient=utils.one_hot_tensor(idx, 1000), retain_graph=True)

    def get_input_gradient(self, idx, sorted_idx=False, backward=True):
        """ The backward pass and return the gradient at input image

        Argument:
            idx (int) - the idx of the class to be localize by GradCAM
            sorted_idx (bool) - if sorted_idx==True, the idx[0] will be the class with highest score,
                idx[1] will be the class with second highest score and so on
            backward (bool) - perform backward pass or not,
                                under normal usecase, it should be true

        Return:
            PIL image - The RGB gradient images generated based on the gradient value
            Numpy array (nxnxc) - The gradient value for each pixel
        """

        #implement sorted_idx !!!

        if backward:
            self.backward(idx)

        # 1x3x224x224 -> 224x224x3
        gradient = self.input.grad.data.cpu().numpy()[0].transpose(1, 2, 0)

        gradient_img_arr = (utils.normalize(gradient)*255).astype('uint8')
        return Image.fromarray(gradient_img_arr), gradient

class GuidedBackpropagation(Backpropagation):
    """
    Guided Backpropagation

    x.grad or img.grad is what we wanted
    GuidedBackprop: input>0 * gradin>0 * gradin on relu.backward
    but original relu had implemented relu gradin = input>0 * gradin
    thus we only need to add gradin>0 * relu gradin

    Reference: https://arxiv.org/abs/1412.6806

    NOTE:
        The gradient on back propagation of the model will be modify, if this is
        not the desired behaeviour, construct this class with a deepcopy of model
    """

    def __init__(self, model, transform, cuda=False):
        super().__init__(model, transform, cuda)

        # define hook function
        def backward_hook(module, grad_input, grad_output):
            # Guided Backpropagation
            # Only allows positive gradient to backflow
            return (torch.clamp(grad_input[0], min=0.0),)

        # register hook function on relu module
        for name, module in self.model.named_modules():
            if isinstance(module, torch.nn.ReLU):
                module.register_backward_hook(backward_hook)

class GuidedGradCAM:
    """
    Guided Grad-CAM
    Use the heatmap of Grad-CAM to produce a localized guided-backprop saliency

    Reference: https://arxiv.org/abs/1610.02391

    NOTE:
        This class is present for completeness and consistancy.
        For general visualization usage, please use the Visualize wrapper class
    """
    def __init__(self, model, transform, target_layer, cuda=False):
        self.model = model
        self.cuda = cuda
        self.transform = transform
        self.target_layer = target_layer

        self.GradCAM = GradCAM(model, transform, target_layer, cuda)
        self.GuidedBackprop = GuidedBackpropagation(copy.deepcopy(model),
                                                    transform, cuda)

    def forward(self, img):
        """ The forward pass
        Argument:
            img (Tensor) - the (unprocessed) input image
        Return:
            Tensor/Dict
        """
        pass

class Visualize:
    """
    Warpper class of all the visualizatiom for efficiency and consistancy

    NOTE:
        I know some part of this class looks weird and redundant but my aims
        is to make different classes share the same model for efficiency
    """

    def __init__(self, model, transform, target_layer, retainModel=True, cuda=False):
        self.model = model
        self.cuda = cuda
        self.transform = transform
        self.target_layer = target_layer

        if retainModel:
            self.model = copy.deepcopy(model)

        self.gradCAM = GradCAM(self.model, self.transform,
                               self.target_layer, self.cuda)
        self.vBackprop = Backpropagation(self.model, self.transform, self.cuda)
        self.gBackprop = GuidedBackpropagation(
            # guided backprop will change the gradient on back prop
            # need a copy of model
            copy.deepcopy(model),
            self.transform,
            self.cuda
        )

    def input_image(self, image):
        """ Supply input image for "visual explanation"

        Argument:
            img (PIL Image) - the (unprocessed) input image

        Return:
            None
        """

        self.image = image
        img_tensor = self.transform(image)
        img_tensor.unsqueeze_(0) # this add a dimension as a dummy "batch"
        img_variable = Variable(img_tensor, requires_grad=True)

        self.input = img_variable
        self.ginput = copy.deepcopy(img_variable)

        self._forwarded = False
        self._forwarded_gbackprop = False
        self._model_idx = None
        self._gmodel_idx = None
        self._gradcam_intensity = None
        self._gradcam_intensity_idx = None
        self._vbackprop_gradient = None
        self._vbackprop_gradient_idx = None
        self._gbackprop_gradient = None
        self._gbackprop_gradient_idx = None

        # self.output = self.model(img_variable)
        # return self.output.data


    def get_prediction_output(self):
        """ Get the prediction output of the model

        Argument:
            None

        Return:
            Tensor - the prediction output of the model
        """

        if self._forwarded:
            return self.output.data
        if self._forwarded_gbackprop:
            return self.output_gbackprop.data

        self.output = self.model(self.input)
        self.gradCAM.input = self.input
        self.vBackprop.input = self.input
        self._forwarded = True

        return self.output.data

    def get_gradcam_intensity(self, idx, sorted_idx=False):
        """ Get the GradCAM intensity value for each pixel

        Argument:
            idx (int) - the idx of the class to be localize by GradCAM
            sorted_idx (bool) - if sorted_idx==True, the idx[0] will be the class with highest score,
                idx[1] will be the class with second highest score and so on

        Return:
            Tensor (size == kernal size of target layer)
                - GradCAM intensity value for each pixel
        """

        if sorted_idx:
            # implement sorted_idx at here to keep idx consistancy
            # idx = sorted_idx(idx)
            pass

        if self._gradcam_intensity_idx == idx:
            return self._gradcam_intensity

        if not self._forwarded:
            self.output = self.model(self.input)
            self.gradCAM.input = self.input
            self.vBackprop.input = self.input
            self._forwarded = True

        if self._model_idx != idx:
            self.model.zero_grad()
            self.output.backward(gradient=utils.one_hot_tensor(idx, 1000), retain_graph=True)
            self._model_idx = idx

        self._gradcam_intensity = self.gradCAM.get_gradcam_intensity(idx, backward=False)
        self._gradcam_intensity_idx = idx;

        return self._gradcam_intensity

    def get_gradcam_heatmap(self, idx, sorted_idx=False):
        """ Get the GradCAM heatmap on the original input image
            and intensity value that is same size as original input image

        Argument:
            idx (int) - the idx of the class to be localize by GradCAM
            sorted_idx (bool) - if sorted_idx==True, the idx[0] will be the class with highest score,
                idx[1] will be the class with second highest score and so on

        Return:
            PIL image - The original image with GradCAM intensity applied to
            Numpy array - The intensity same size as original image (range: [0-1])
        """

        intensity = self.get_gradcam_intensity(idx, sorted_idx)
        return self.gradCAM.apply_color_map(intensity, self.image)

    def get_vanilla_backprop_gradient(self, idx, sorted_idx=False):
        """

        Return:
            PIL image - The gradient image
            Numpy array - The gradient of original image (range: [0-1])
        """
        if sorted_idx:
            # implement sorted_idx at here to keep idx consistancy
            # idx = sorted_idx(idx)
            pass

        if self._vbackprop_gradient_idx == idx:
            return self._vbackprop_gradient

        if not self._forwarded:
            self.output = self.model(self.input)
            self.gradCAM.input = self.input
            self.vBackprop.input = self.input
            self._forwarded = True

        if self._model_idx != idx:
            self.model.zero_grad()
            self.output.backward(gradient=utils.one_hot_tensor(idx, 1000), retain_graph=True)
            self._model_idx = idx

        self._vbackprop_gradient = self.vBackprop.get_input_gradient(idx, backward=False)
        self._vbackprop_gradient_idx = idx;

        return self._vbackprop_gradient

    # def get_vanilla_backprop_saliency(self, idx, sorted_idx=False):
    #     gradient = self.get_vanilla_backprop_gradient(idx, sorted_idx)
    #
    #     return utils.normalize(gradient)

    def get_guided_backprop_gradient(self, idx, sorted_idx=False):
        if sorted_idx:
            # implement sorted_idx at here to keep idx consistancy
            # idx = sorted_idx(idx)
            pass

        if self._gbackprop_gradient_idx == idx:
            return self._gbackprop_gradient

        if not self._forwarded_gbackprop:
            self.output_gbackprop = self.gBackprop.model(self.ginput)
            self.gBackprop.input = self.ginput
            self._forwarded_gbackprop = True

        if self._gmodel_idx != idx:
            self.gBackprop.model.zero_grad()
            self.output_gbackprop.backward(gradient=utils.one_hot_tensor(idx, 1000), retain_graph=True)
            self._gmodel_idx = idx

        self._gbackprop_gradient = self.gBackprop.get_input_gradient(idx, backward=False)
        self._gbackprop_gradient_idx = idx;

        return self._gbackprop_gradient

    # def get_guided_backprop_saliency(self, idx, sorted_idx=False):
    #     gradient = self.get_guided_backprop_gradient(idx, sorted_idx)
    #
    #     return utils.normalize(gradient)
    #
    def get_guided_gramcam_saliency(self, idx, sorted_idx=False):
        _, gradcam = self.get_gradcam_heatmap(idx, sorted_idx)
        _, gradient = self.get_guided_backprop_gradient(idx, sorted_idx)

        # gradient = utils.normalize(gradient) # this make the background black in guided gradcam
        result = (gradient.transpose(2, 0, 1)*gradcam).transpose(1, 2, 0)
        result = utils.normalize(result)
        return Image.fromarray((result*255).astype('uint8')), result

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 2:
        print("usage: python visualize.py path/to/image")
        exit()

    model_name = 'resnet152'

    resnet = torchvision.models.__dict__[model_name](pretrained=True)
    input_size = utils.get_input_size(model_name)
    target_layer = utils.get_conv_layer(model_name)
    preprocess = transforms.Compose([
       transforms.Resize(input_size),
       transforms.ToTensor(),
       transforms.Normalize(
           mean=[0.485, 0.456, 0.406],
           std=[0.229, 0.224, 0.225]
        )
    ])
    class_name = json.load(open('data/class_name.json', 'r'))

    img_pil = Image.open(sys.argv[1])
    # img_pil = img_pil.resize((224, 224))

    visualizer = Visualize(resnet, preprocess, target_layer, retainModel=False)

    visualizer.input_image(img_pil)
    x = visualizer.get_prediction_output()
    score = x.cpu().numpy()[0]

    # get the top 3 prediction
    print("Top 3 prediction")
    for i in range(3):
        idx = score.argmax()
        print(idx, score[idx], class_name[idx])

        img = [
            visualizer.get_gradcam_heatmap(idx)[0],
            visualizer.get_guided_backprop_gradient(idx)[0],
            visualizer.get_vanilla_backprop_gradient(idx)[0],
            visualizer.get_guided_gramcam_saliency(idx)[0]
        ]
        title = ["Grad-CAM", "Guided Backpropagation", "Backpropagation", "Guided Grad-CAM"]
        fig = PLT.figure(class_name[idx].split(",")[0])

        for i in range(4):
            ax = fig.add_subplot(221+i)
            ax.axis('off')
            ax.imshow(img[i])
            ax.set_title(title[i])

        PLT.suptitle(class_name[idx]+" Score: "+str(x[0][idx])[:5], fontsize=18)
        PLT.show()

        score[idx] = -1000
